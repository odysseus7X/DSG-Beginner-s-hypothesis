{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "yogeshwartheboss_cholect50_path = kagglehub.dataset_download('yogeshwartheboss/cholect50')\n",
    "\n",
    "print('Data source import complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import platform\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "data_dir= '/root/.cache/kagglehub/datasets/yogeshwartheboss/cholect50/versions/1/CholecT50'\n",
    "batch_size= 32\n",
    "epochs= 40\n",
    "OUT_HEIGHT = 8\n",
    "OUT_WIDTH  = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mymodel(nn.Module):\n",
    "    def __init__(self, num_tool=6, num_verb=10, num_target=15, num_triplet=100, layer_size=5, num_class=100, use_ln=True):\n",
    "        super(mymodel, self).__init__()\n",
    "        self.basemodel= Backbone()\n",
    "        self.model1= Model1(num_tool)\n",
    "        self.model2= Model2(num_tool, num_verb, num_target)\n",
    "        self.model3= Model3(num_verb)\n",
    "        self.channelattention = nn.ModuleList([ChannelAttention() for i in range(layer_size)])\n",
    "        self.ffn = nn.ModuleList([FFN() for i in range(layer_size)])\n",
    "        self.classifier = Classifier(num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_i, x_t, x_v = self.basemodel(x)\n",
    "        H_i = self.model1(x_i)\n",
    "        H_t = self.model2(x_t, H_i[0])\n",
    "        H_v = self.model3(x_v, H_t[0])\n",
    "        for C, F in zip(self.channelattention, self.ffn):\n",
    "            X = C(H_i[0], H_t[0], H_v[0])\n",
    "            X = F(X)\n",
    "        logits = self.classifier(X)\n",
    "        return H_i, H_t, H_v, logits\n",
    "\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Backbone, self).__init__()\n",
    "        self.output_feature = {} \n",
    "        self.backbone      = models.resnet18(pretrained=True)\n",
    "        self.increase_resolution()\n",
    "        self.backbone.layer1[1].bn2.register_forward_hook(self.get_activation('low_level_feature'))\n",
    "        self.backbone.layer2[1].bn2.register_forward_hook(self.get_activation('mid_level_feature'))\n",
    "        self.backbone.layer4[1].bn2.register_forward_hook(self.get_activation('high_level_feature'))       \n",
    "\n",
    "    def increase_resolution(self):  \n",
    "        global OUT_HEIGHT, OUT_WIDTH\n",
    "        self.backbone.layer2[0].conv1.stride = (1,1)\n",
    "        self.backbone.layer2[0].downsample[0].stride=(1,1)  \n",
    "        self.backbone.layer4[0].conv1.stride = (1,1)\n",
    "        self.backbone.layer4[0].downsample[0].stride=(1,1)\n",
    "        OUT_HEIGHT *= 4\n",
    "        OUT_WIDTH  *= 4\n",
    "        print(\"using high resolution output ({}x{})\".format(OUT_HEIGHT,OUT_WIDTH))      \n",
    "\n",
    "    def get_activation(self, layer_name):\n",
    "        def hook(module, input, output):\n",
    "            self.output_feature[layer_name] = output\n",
    "        return hook\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _ = self.backbone(x)\n",
    "        return self.output_feature['high_level_feature'], self.output_feature['mid_level_feature'], self.output_feature['low_level_feature']\n",
    "\n",
    "class Model1(nn.Module):\n",
    "    def __init__(self, num_tool):\n",
    "        super(Model1, self).__init__()\n",
    "        self.Conv1 = nn.Conv2d(in_channels= 512, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.cam   = nn.Conv2d(in_channels=64, out_channels=num_tool, kernel_size=1)\n",
    "        self.elu   = nn.ELU()\n",
    "        self.bn    = nn.BatchNorm2d(64)\n",
    "        self.gmp   = nn.AdaptiveMaxPool2d((1,1))\n",
    "\n",
    "    def forward (self, x):\n",
    "        x = self.Conv1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.elu(x)\n",
    "        cam = self.cam(x)\n",
    "        logits  = self.gmp(cam).squeeze(-1).squeeze(-1)\n",
    "        return cam, logits\n",
    "\n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, num_tool, num_verb, num_target):\n",
    "        super(Model2, self).__init__()\n",
    "        self.Conv1 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1, stride=(2,2))\n",
    "        self.Conv2 = nn.Conv2d(in_channels=70, out_channels=32, kernel_size=1, padding=0)\n",
    "        self.cam   = nn.Conv2d(in_channels=32, out_channels=num_target, kernel_size=1)\n",
    "        self.elu   = nn.ELU()\n",
    "        self.bn1    = nn.BatchNorm2d(64)\n",
    "        self.bn2    = nn.BatchNorm2d(32)\n",
    "        self.gmp   = nn.AdaptiveMaxPool2d((1,1))\n",
    "    \n",
    "    def get_inp (self, raw_t, cam):\n",
    "        M2_inp = torch.cat((raw_t, cam), dim =1)\n",
    "        return M2_inp\n",
    "    \n",
    "    def forward (self, x, cam):\n",
    "        x = self.Conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.get_inp(x, cam)\n",
    "        x = self.Conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.elu(x)\n",
    "        cam_t = self.cam(x)\n",
    "        logits_t  = self.gmp(cam_t).squeeze(-1).squeeze(-1)\n",
    "        return cam_t, logits_t\n",
    "\n",
    "class Model3(nn.Module):\n",
    "    def __init__(self,num_verb):\n",
    "        super(Model3, self).__init__()\n",
    "        self.Conv1 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1, stride=(2,2))\n",
    "        self.Conv2 = nn.Conv2d(in_channels=47, out_channels=16, kernel_size=1, padding=0)\n",
    "        self.cam   = nn.Conv2d(in_channels=16, out_channels=num_verb, kernel_size=1)\n",
    "        self.elu   = nn.ELU()\n",
    "        self.bn1    = nn.BatchNorm2d(32)\n",
    "        self.bn2    = nn.BatchNorm2d(16)\n",
    "        self.gmp   = nn.AdaptiveMaxPool2d((1,1))\n",
    "    \n",
    "    def get_inp (self, raw_v, cam_t):\n",
    "        M3_inp = torch.cat((raw_v, cam_t), dim =1)\n",
    "        return M3_inp\n",
    "\n",
    "    def forward (self, x, cam):\n",
    "        x = self.Conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.get_inp(x, cam)\n",
    "        x = self.Conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.elu(x) \n",
    "        cam_v = self.cam(x)\n",
    "        logits_v  = self.gmp(cam_v).squeeze(-1).squeeze(-1)\n",
    "        return cam_v, logits_v\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, num_class=100):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d(1) \n",
    "        \n",
    "        self.fc1 = nn.Conv2d(in_channels=31, out_channels =128, kernel_size=1, stride=1, padding=0)\n",
    "        self.fc2 = nn.Conv2d(in_channels=128, out_channels=num_class, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, cam_i, cam_t, cam_v):\n",
    "        x = torch.cat((cam_i, cam_t, cam_v), dim =1 )\n",
    "        gap = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.fc1(gap))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        attention_weights = self.sigmoid(x)\n",
    "        \n",
    "        return x * attention_weights\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, num_class=100, use_ln=True):\n",
    "        super(FFN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_class, out_channels=num_class, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=num_class, out_channels=num_class, kernel_size=1) \n",
    "        self.elu1  = nn.ELU() \n",
    "        self.elu2  = nn.ELU()    \n",
    "        self.bn1   = nn.BatchNorm2d(num_class)    \n",
    "        self.bn2   = nn.BatchNorm2d(num_class)\n",
    "        self.ln    = nn.BatchNorm2d(num_class)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x  = self.elu1(self.bn1(self.conv1(inputs)))\n",
    "        x  = self.elu2(self.bn2(self.conv2(x)))\n",
    "        x  = self.ln(x + inputs.clone())\n",
    "        return x\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_class=100):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.gmp = nn.AdaptiveMaxPool2d((1,1)) \n",
    "        self.mlp = nn.Linear(in_features=num_class, out_features=num_class)     \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.gmp(inputs).squeeze(-1).squeeze(-1)\n",
    "        y = self.mlp(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bbox_for_instrument(cam):\n",
    "    # Normalize cam values to range [0, 1]\n",
    "    cam_normalized = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "    \n",
    "    threshold = 0.5\n",
    "    binary_mask = cam_normalized > threshold\n",
    "    \n",
    "    # Convert binary mask to uint8 format, moving the tensor to CPU\n",
    "    binary_mask_uint8 = (binary_mask.cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    # Ensure it's a 2D array (single channel)\n",
    "    if binary_mask_uint8.ndim != 2:\n",
    "        raise ValueError(\"binary_mask_uint8 must be single-channel (2D array).\")\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary_mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    most_activated_bbox = None\n",
    "    max_activation_score = -1\n",
    "    max_poss_activation_score = -1  # Initialize at the start\n",
    "\n",
    "    for contour in contours:\n",
    "        mask = np.zeros_like(cam.detach().cpu(), dtype=np.uint8)\n",
    "        score = np.ones_like(cam.detach().cpu(), dtype=np.uint8)\n",
    "        cv2.drawContours(mask, [contour], -1, 255, thickness=-1)\n",
    "        \n",
    "        activation_score = torch.sum(cam_normalized[mask > 0].detach().cpu())\n",
    "        max_poss_activation_score = np.sum(score[mask > 0])  # Possible max score in this region\n",
    "        \n",
    "        if activation_score > max_activation_score:\n",
    "            max_activation_score = activation_score\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            most_activated_bbox = (x, y, x + w, y + h)\n",
    "\n",
    "    max_activation_score = max_activation_score / max_poss_activation_score\n",
    "\n",
    "    # Rescaling box coordinates according to the original image size\n",
    "    original_width, original_height = 854, 480\n",
    "    scale_x = original_width / cam.shape[1]\n",
    "    scale_y = original_height / cam.shape[0]\n",
    "\n",
    "    # Rescale bounding box\n",
    "    if most_activated_bbox:\n",
    "        x, y, w, h = most_activated_bbox\n",
    "        rescaled_box = [\n",
    "            (int(x * scale_x), int(y * scale_y), int(w * scale_x), int(h * scale_y))\n",
    "        ]\n",
    "    else:\n",
    "        rescaled_box = []\n",
    "\n",
    "    return rescaled_box, max_activation_score\n",
    "\n",
    "def compute_act_scores(cams):\n",
    "    # Initialize act_scores as a 2D array (32, 6)\n",
    "    act_scores = np.zeros((cams.shape[0], 6))\n",
    "    \n",
    "    # Loop through each cam (assuming cams shape is [32, 6, H, W])\n",
    "    for i in range(cams.shape[0]):\n",
    "        for j in range(6):\n",
    "            bbox, act_score = generate_bbox_for_instrument(cams[i][j])\n",
    "            act_scores[i, j] = act_score  # Store the activation score\n",
    "    return act_scores\n",
    "\n",
    "\n",
    "\n",
    "def make_dic(predicted_data):\n",
    "        dic = {}\n",
    "\n",
    "        for frame_id in range(len(predicted_data[\"triplet\"])):\n",
    "            dic[str(frame_id)] = {}\n",
    "            new_dic = {}\n",
    "            new_dic[\"recognition\"] = predicted_data[\"triplet\"][frame_id]\n",
    "\n",
    "            threshold = 0.5\n",
    "            activated_classes = (new_dic[\"recognition\"] > threshold).nonzero(as_tuple=True)[0]\n",
    "            predicted_labels = np.zeros(100)\n",
    "\n",
    "            activated_classes_instrument = (predicted_data[\"instrument\"] > threshold).nonzero(as_tuple=True)[0]\n",
    "        \n",
    "            for j in activated_classes:\n",
    "                predicted_labels[j] = 1        \n",
    "\n",
    "            new_dic[\"detection\"] = []\n",
    "\n",
    "            for p in activated_classes:\n",
    "            \n",
    "                if (p>=0) and (p<=21 or p==94): g=0\n",
    "                elif (p>=22) and (p<=45 or p==95): g=1\n",
    "                elif (p>=46) and (p<=64 or p==96): g=2\n",
    "                elif (p>=65) and (p<=76 or p==97): g=3\n",
    "                elif (p>=77) and (p<=81 or p==98): g=4\n",
    "                elif (p>=82) and (p<=93 or p==99): g=5\n",
    "\n",
    "                one_more_dic = {}\n",
    "                one_more_dic[\"triplet\"] = p\n",
    "            \n",
    "                bbox = generate_bbox_for_instrument(predicted_data[\"CAM_instrument\"][frame_id][g])\n",
    "            \n",
    "                one_more_dic[\"instrument\"] = [g, predicted_data[\"CAM_instrument\"][frame_id][g], bbox[0], bbox[1], bbox[2], bbox[3]]\n",
    "                new_dic[\"detection\"].append(one_more_dic)\n",
    "\n",
    "        return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CholecT50():\n",
    "    def __init__(self, dataset_dir, \n",
    "                 augmentation_list=['original', 'vflip', 'hflip', 'contrast', 'rot90']):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.train_records = ['VID{}'.format(str(v).zfill(2)) for v in [1, 2, 4, 5, 6, 8, 10, 12]]\n",
    "        self.val_records   = ['VID{}'.format(str(v).zfill(2)) for v in [13, 14]]\n",
    "        self.test_records  = ['VID{}'.format(str(v).zfill(2)) for v in [92, 96, 103, 110, 111]]\n",
    "        self.augmentations = {\n",
    "            'original': self.no_augmentation,\n",
    "            'vflip': transforms.RandomVerticalFlip(0.4),\n",
    "            'hflip': transforms.RandomHorizontalFlip(0.4),\n",
    "            'contrast': transforms.ColorJitter(brightness=0.1, contrast=0.2, saturation=0, hue=0),\n",
    "            'rot90': transforms.RandomRotation(90,expand=True),\n",
    "            'brightness': transforms.RandomAdjustSharpness(sharpness_factor=1.6, p=0.5),\n",
    "            'autocontrast': transforms.RandomAutocontrast(p=0.5),\n",
    "        }\n",
    "        self.augmentation_list = []\n",
    "        for aug in augmentation_list:\n",
    "            self.augmentation_list.append(self.augmentations[aug])\n",
    "        trainform, testform = self.transform()\n",
    "        self.build_train_dataset(trainform)\n",
    "        self.build_val_dataset(trainform)\n",
    "        self.build_test_dataset(testform)\n",
    "\n",
    "    def no_augmentation(self, x):\n",
    "        return x\n",
    "   \n",
    "    def transform(self):\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        op_test   = [transforms.Resize((256, 448)), transforms.ToTensor(), normalize,]\n",
    "        op_train  = [transforms.Resize((256, 448))] + self.augmentation_list + [transforms.Resize((256, 448)), transforms.ToTensor(), normalize,]\n",
    "        testform  = transforms.Compose(op_test)\n",
    "        trainform = transforms.Compose(op_train)\n",
    "        return trainform, testform\n",
    "\n",
    "    def build_train_dataset(self, transform):\n",
    "        iterable_dataset = []\n",
    "        for video in self.train_records:\n",
    "            dataset = T50(img_dir = os.path.join(self.dataset_dir, 'videos', video), \n",
    "                        label_file = os.path.join(self.dataset_dir, 'labels', '{}.json'.format(video)), \n",
    "                        transform=transform)\n",
    "            iterable_dataset.append(dataset)\n",
    "        self.train_dataset = ConcatDataset(iterable_dataset)\n",
    "    \n",
    "    def build_val_dataset(self, transform):\n",
    "        iterable_dataset = []\n",
    "        for video in self.val_records:\n",
    "            dataset = T50(img_dir = os.path.join(self.dataset_dir, 'videos', video), \n",
    "                        label_file = os.path.join(self.dataset_dir, 'labels', '{}.json'.format(video)), \n",
    "                        transform=transform)\n",
    "            iterable_dataset.append(dataset)\n",
    "        self.val_dataset = ConcatDataset(iterable_dataset)\n",
    "\n",
    "    def build_test_dataset(self, transform):\n",
    "        iterable_dataset = []\n",
    "        for video in self.test_records:\n",
    "            dataset = T50(img_dir = os.path.join(self.dataset_dir, 'videos', video), \n",
    "                        label_file = os.path.join(self.dataset_dir, 'labels', '{}.json'.format(video)), \n",
    "                        transform=transform)\n",
    "            iterable_dataset.append(dataset)\n",
    "        self.test_dataset = iterable_dataset\n",
    "\n",
    "    def build(self):\n",
    "        return   (self.train_dataset, self.val_dataset, self.test_dataset)\n",
    "\n",
    "\n",
    "class T50(Dataset):\n",
    "    def __init__(self, img_dir, label_file, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_file_path = label_file\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    def __len__(self):\n",
    "        data_json = json.load(open(self.label_file_path, \"r\"))\n",
    "        return len(data_json[\"annotations\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        basename = \"{}.png\".format(str(index).zfill(6))\n",
    "        img_path = os.path.join(self.img_dir, basename)\n",
    "        imagey    = Image.open(img_path)\n",
    "        image = self.transform(imagey)\n",
    "        \n",
    "        with open(self.label_file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            array = data[\"annotations\"][str(index)]\n",
    "            triplet_label = torch.zeros(100, dtype=torch.int)\n",
    "            instrument_label = torch.zeros(6, dtype=torch.int)\n",
    "            verb_label = torch.zeros(10, dtype=torch.int)\n",
    "            target_label = torch.zeros(15, dtype=torch.int)\n",
    "            \n",
    "            for a in array:\n",
    "                triplet_label[a[0]] = 1 if a[0]!= -1 else 0\n",
    "                instrument_label[a[1]] = 1 if a[1]!= -1 else 0\n",
    "                verb_label[a[7]] = 1 if a[7]!= -1 else 0\n",
    "                target_label[a[8]] = 1 if a[8]!= -1 else 0\n",
    "            \n",
    "        return image, (triplet_label, instrument_label, verb_label, target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, activation, loss_fn_i, loss_fn_v, loss_fn_t, loss_fn_ivt, loss_association, epoch):\n",
    "    start = time.time()\n",
    "    running_loss=0.0\n",
    "    for batch, (img, (y_triplet, y_instrument, y_verb, y_target)) in enumerate(dataloader):\n",
    "        img, y_triplet, y_instrument, y_verb, y_target = img.cuda(), y_triplet.cuda(), y_instrument.cuda(), y_verb.cuda(), y_target.cuda()\n",
    "        model.train()\n",
    "        instrument, target, verb, triplet = model(img)\n",
    "        cam_i, logit_i  = instrument\n",
    "        cam_v, logit_v  = verb\n",
    "        cam_t, logit_t  = target\n",
    "        logit_ivt = triplet\n",
    "\n",
    "        act_values = compute_act_scores(cam_i)\n",
    "        act_values_tensor = torch.from_numpy(act_values)\n",
    "\n",
    "        \n",
    "        loss_i          = loss_fn_i(logit_i, y_instrument.float())\n",
    "        loss_v          = loss_fn_v(logit_v, y_verb.float())\n",
    "        loss_t          = loss_fn_t(logit_t, y_target.float())\n",
    "        loss_ivt        = loss_fn_ivt(logit_ivt, y_triplet.float())\n",
    "        loss_ass        = loss_association(act_values_tensor.cuda(), y_instrument.float())\n",
    "\n",
    "        loss            = (loss_i) + (loss_v) + (loss_t) + loss_ivt + loss_ass\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if batch % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{batch+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Average Loss: {epoch_loss:.4f}\")\n",
    "    print(f'completed | Losses => i: [{loss_i.item():.4f}] v: [{loss_v.item():.4f}] t: [{loss_t.item():.4f}] ivt: [{loss_ivt.item():.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, activation, final_eval=True):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    predictions = {\"instrument\": [], \"verb\": [], \"target\": [], \"triplet\": [], \"CAM_instrument\":[]}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (img, (y_triplet, y_instrument, y_verb, y_target)) in enumerate(dataloader):\n",
    "            img, y_triplet, y_instrument, y_verb, y_target = img.cuda(), y_triplet.cuda(), y_instrument.cuda(), y_verb.cuda(), y_target.cuda()            \n",
    "            model.eval()  \n",
    "            instrument, target, verb, triplet = model(img)\n",
    "            if final_eval:\n",
    "                cam_i, logit_i = instrument\n",
    "                cam_v, logit_v = verb\n",
    "                cam_t, logit_t = target\n",
    "\n",
    "                predictions[\"instrument\"].append(activation(logit_i).cpu())\n",
    "                predictions[\"verb\"].append(activation(logit_v).cpu())\n",
    "                predictions[\"target\"].append(activation(logit_t).cpu())\n",
    "                predictions[\"CAM_instrument\"].append(cam_i.cpu())\n",
    "                \n",
    "            predictions[\"triplet\"].append(activation(triplet).cpu())\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_class = CholecT50( \n",
    "            dataset_dir=data_dir,\n",
    "            augmentation_list=['original', 'vflip', 'hflip', 'contrast', 'rot90'],\n",
    "            )\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = dataloader_class.build()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, prefetch_factor=3*batch_size, num_workers=2, pin_memory=True, persistent_workers=True, drop_last=False)\n",
    "val_dataloader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, prefetch_factor=3*batch_size, num_workers=2, pin_memory=True, persistent_workers=True, drop_last=False)\n",
    "\n",
    "test_dataloaders = []\n",
    "for video_dataset in test_dataset:\n",
    "    test_dataloader = DataLoader(video_dataset, batch_size=batch_size, shuffle=False, prefetch_factor=3*batch_size, num_workers=2, pin_memory=True, persistent_workers=True, drop_last=False)\n",
    "    test_dataloaders.append(test_dataloader)\n",
    "print(\"Dataset loaded ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mymodel(use_ln=True).cuda()\n",
    "\n",
    "activation  = nn.Sigmoid()\n",
    "loss_fn_i   = nn.BCEWithLogitsLoss()\n",
    "loss_fn_v   = nn.BCEWithLogitsLoss()\n",
    "loss_fn_t   = nn.BCEWithLogitsLoss()\n",
    "loss_fn_ivt = nn.BCEWithLogitsLoss()\n",
    "loss_association = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, epochs):\n",
    "    try:\n",
    "        print(\"Traning | epoch {}\".format(epoch), end=\" | \")  \n",
    "        train_loop(train_dataloader, model, activation, loss_fn_i, loss_fn_v, loss_fn_t, loss_fn_ivt, loss_association, epoch) \n",
    "           \n",
    "    except KeyboardInterrupt:\n",
    "        print(f'>> Process cancelled by user ...')   \n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_index = [\"VID92\", \"VID96\", \"VID103\", \"VID110\", \"VID111\"]\n",
    "i=0\n",
    "dic_data = {}\n",
    "\n",
    "for test_dataloader in test_dataloaders:\n",
    "    predicted_data = test_loop(test_dataloader, model, activation, final_eval=True)\n",
    "    dic_data[vid_index[i]] = make_dic(predicted_data)\n",
    "        \n",
    "    i+=1\n",
    "\n",
    "with open(f\"mymodel.json\", 'w') as json_file:\n",
    "    json.dump(dic_data, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
